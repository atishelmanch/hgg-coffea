{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ea72b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgray/miniconda3/envs/hgg-work/lib/python3.8/site-packages/coffea/nanoevents/schemas/nanoaod.py:193: RuntimeWarning: Missing cross-reference index for FatJet_genJetAK8Idx => GenJetAK8\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# open files / get meta conditions\n",
    "from coffea import nanoevents\n",
    "events = nanoevents \\\n",
    "         .NanoEventsFactory \\\n",
    "         .from_root(\n",
    "             '../data/954C2B25-9CC5-004D-9CCE-FA674345E337.root',\n",
    "             metadata = {\"dataset\": \"test\"}\n",
    "         ).events()\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "metaconditions_file = \"../Era2017_legacy_v1.json\"\n",
    "with open(metaconditions_file) as f:\n",
    "    metaconditions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377fa39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coffea\n",
    "from coffea import hist, processor\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import functools as ft\n",
    "import operator as op\n",
    "import os\n",
    "import shutil as shu\n",
    "import pathlib as pl\n",
    "\n",
    "class DYStudiesProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, metaconditions, do_systematics=False, apply_trigger=False, output_location=None):\n",
    "        self.meta = metaconditions\n",
    "        self.do_systematics = do_systematics\n",
    "        self.apply_trigger = apply_trigger\n",
    "        self.output_location = output_location\n",
    "        self.trigger_group = \".*DoubleEG.*\"\n",
    "        self.analysis = \"mainAnalysis\"\n",
    "\n",
    "        # diphoton preselection cuts\n",
    "        self.min_pt_photon = 25.0\n",
    "        self.min_pt_lead_photon = 35.0\n",
    "        self.min_mvaid = -0.9\n",
    "        self.max_sc_eta = 2.5\n",
    "        self.gap_barrel_eta = 1.4442\n",
    "        self.gap_endcap_eta = 1.566\n",
    "        self.max_hovere = 0.08\n",
    "        self.min_full5x5_r9 = 0.8\n",
    "        self.max_chad_iso = 20.0\n",
    "        self.max_chad_rel_iso = 0.3\n",
    "\n",
    "        self.prefixes = {\"0\": \"lead\", \"1\": \"sublead\"}\n",
    "        \n",
    "    def photon_preselection(self, photons):\n",
    "        photon_abs_eta = np.abs(photons.eta)\n",
    "        return photons[  (photons.pt > self.min_pt_photon)\n",
    "                       & (photon_abs_eta < self.max_sc_eta)\n",
    "                       & ((photon_abs_eta < self.gap_barrel_eta) | (photon_abs_eta > self.gap_endcap_eta))\n",
    "                       & (photons.mvaID > self.min_mvaid)\n",
    "                       & (photons.hoe < self.max_hovere)\n",
    "                       & (  (photons.r9 > self.min_full5x5_r9)\n",
    "                          | (photons.pfRelIso03_chg < self.max_chad_iso)\n",
    "                          | (photons.pfRelIso03_chg/photons.pt < self.max_chad_rel_iso))]\n",
    "\n",
    "    def diphoton_list_to_pandas(self, diphotons):\n",
    "        output = pd.DataFrame()\n",
    "        for field in ak.fields(diphotons):\n",
    "            prefix = self.prefixes.get(field, \"\")\n",
    "            if len(prefix) > 0:\n",
    "                for subfield in ak.fields(diphotons[field]):\n",
    "                    output[f\"{prefix}_{subfield}\"] = ak.to_numpy(diphotons[field][subfield])\n",
    "            else:\n",
    "                output[field] = ak.to_numpy(diphotons[field])\n",
    "        return output\n",
    "\n",
    "    def dump_pandas(self, pddf, fname, location, subdirs=[]):\n",
    "        xrd_prefix = 'root://'\n",
    "        xrootd = False\n",
    "        if xrd_prefix in location:\n",
    "            try:\n",
    "                import XRootD\n",
    "                import XRootD.client\n",
    "                xrootd = True\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Install XRootD python bindings with: conda install -c conda-forge xroot\"\n",
    "                )\n",
    "        local_file = os.path.join(\".\", fname)\n",
    "        subdirs = \"/\".join(subdirs) if xrootd else os.path.sep.join(subdirs)\n",
    "        destination = locations + subdirs + f\"/{fname}\" if xrootd else os.path.join(location, os.path.join(subdirs, fname))\n",
    "        pddf.to_parquet(local_file)\n",
    "        if xrootd:\n",
    "            pfx_len = len(xrd_prefix)\n",
    "            client = XRootD.client.FileSystem(location[:location[pfx_len:].find('/') + pfx_len])\n",
    "            status = client.copy(local_file, destination)\n",
    "            assert status[0].ok\n",
    "        else:\n",
    "            dirname = os.path.dirname(destination)\n",
    "            if not os.path.exists(dirname):\n",
    "                pl.Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "            shu.copy(local_file, destination)\n",
    "            assert os.path.isfile(destination)\n",
    "        \n",
    "            \n",
    "    \n",
    "    def process(self, events):\n",
    "\n",
    "        # data or monte carlo?\n",
    "        data_kind = \"mc\" if \"GenPart\" in ak.fields(events) else \"data\"\n",
    "\n",
    "        # met filters\n",
    "        met_filters = self.meta[\"flashggMetFilters\"][data_kind]\n",
    "        filtered = ft.reduce(op.and_, (events.Flag[metfilter.split(\"_\")[-1]] for metfilter in met_filters))\n",
    "\n",
    "        triggered = ak.ones_like(filtered)\n",
    "        if self.apply_trigger:\n",
    "            triggers = self.meta[\"TriggerPaths\"][self.trigger_group][self.analysis]\n",
    "            triggered = ft.reduce(op.or_, (events.HLT[trigger[4:-1]] for trigger in triggers))\n",
    "        \n",
    "        # apply met filters and triggers to data\n",
    "        events = events[filtered & triggered]\n",
    "\n",
    "        # photon preselection\n",
    "        photons = self.photon_preselection(events.Photon)\n",
    "        # sort photons in each event descending in pt\n",
    "        # make descending-pt combinations of photons\n",
    "        photons = photons[ak.argsort(photons.pt, ascending=False)]\n",
    "        diphotons = ak.combinations(photons, 2)\n",
    "        # the remaining cut is to select the leading photons\n",
    "        # the previous sort assures the order\n",
    "        diphotons = diphotons[diphotons[\"0\"].pt > self.min_pt_lead_photon]\n",
    "\n",
    "        # now turn the diphotons into candidates with four momenta and such\n",
    "        diphoton_4mom = diphotons[\"0\"] + diphotons[\"1\"]\n",
    "        diphotons[\"pt\"] = diphoton_4mom.pt\n",
    "        diphotons[\"eta\"] = diphoton_4mom.eta\n",
    "        diphotons[\"phi\"] = diphoton_4mom.phi\n",
    "        diphotons[\"mass\"] = diphoton_4mom.mass\n",
    "        diphotons = ak.with_name(diphotons, \"PtEtaPhiMCandidate\")\n",
    " \n",
    "        # arbitrate diphotons\n",
    "        diphotons = diphotons[ak.argsort(diphotons.pt, ascending=False)]\n",
    "        diphotons = ak.firsts(diphotons)\n",
    "\n",
    "        # annotate diphotons with event information\n",
    "        diphotons[\"event\"] = events.event\n",
    "        diphotons[\"lumi\"] = events.luminosityBlock\n",
    "        diphotons[\"run\"] = events.run\n",
    "        \n",
    "        # drop events without a preselected diphoton candidate\n",
    "        diphotons = diphotons[~ak.is_none(diphotons)]\n",
    "        \n",
    "        if self.output_location is not None:\n",
    "            df = self.diphoton_list_to_pandas(diphotons)\n",
    "            fname = events.behavior[\"__events_factory__\"]._partition_key.replace(\"/\", \"_\") + \".parquet\"\n",
    "            subdirs = []\n",
    "            if 'dataset' in events.metadata:\n",
    "                subdirs.append(events.metadata[\"dataset\"])\n",
    "            self.dump_pandas(df, fname, self.output_location, subdirs)\n",
    "        \n",
    "        return {\n",
    "            \n",
    "        }\n",
    "\n",
    "    def postprocess(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows import DYStudiesProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d00c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "dystudies = DYStudiesProcessor(metaconditions, False, True, './outputs/')\n",
    "\n",
    "histos = dystudies.process(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a00af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ak.ones_like(ak.flatten(events.Photon.pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff6d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import awkward as ak\n",
    "\n",
    "x = pd.read_parquet('./outputs/test')\n",
    "\n",
    "y = ak.from_arrow(pa.Table.from_pandas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3862b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# di-photon MVA\n",
    "import xgboost as xg\n",
    "\n",
    "min_diphoton_mass = 100\n",
    "max_diphoton_mass = 180\n",
    "\n",
    "model = xg.Booster()\n",
    "model.load_model('aux-data/altDiphoModel_coffea.model')\n",
    "\n",
    "# get the number of diphotons per row\n",
    "# and save for re-wrapping xgb outputs\n",
    "counts = ak.num(diphotons, axis=1)\n",
    "\n",
    "# extract diphoton vars into flat lists\n",
    "dipho_leadIDMVA = ak.flatten(diphotons[\"0\"].mvaID)\n",
    "dipho_subleadIDMVA = ak.flatten(diphotons[\"1\"].mvaID)\n",
    "dipho_lead_ptoM = ak.flatten(diphotons[\"0\"].pt / diphotons.mass)\n",
    "dipho_sublead_ptoM = ak.flatten(diphotons[\"1\"].pt / diphotons.mass)\n",
    "dipho_lead_eta = ak.flatten(diphotons[\"0\"].eta)\n",
    "dipho_sublead_eta = ak.flatten(diphotons[\"1\"].eta)\n",
    "\n",
    "diphoVars  = ['dipho_leadIDMVA', 'dipho_subleadIDMVA', 'dipho_lead_ptoM', \n",
    "              'dipho_sublead_ptoM', 'dipho_leadEta', 'dipho_subleadEta', \n",
    "              'CosPhi', 'vtxprob', 'sigmarv', 'sigmawv']\n",
    "allVars = diphoVars + [\"dipho_mass\"]\n",
    "\n",
    "f = uproot.open('data/ggH_powheg_UL_2017.root')\n",
    "tree = f['vbfTagDumper/trees/ggh_125_13TeV_GeneralDipho']\n",
    "arrays = tree.arrays(allVars, how=dict)\n",
    "\n",
    "mask = (  (arrays[\"dipho_mass\"]> min_diphoton_mass) & (arrays[\"dipho_mass\"]< max_diphoton_mass) \n",
    "        & (arrays[\"dipho_leadIDMVA\"]>-0.9) & (arrays[\"dipho_subleadIDMVA\"]>-0.9) \n",
    "        & (arrays[\"dipho_lead_ptoM\"]>0.333) & (arrays[\"dipho_sublead_ptoM\"]>0.25))\n",
    "\n",
    "x = np.column_stack((ak.to_numpy(arrays[var][mask]) for var in diphoVars))[:100]\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "diphoMatrix = xg.DMatrix(x, feature_names=diphoVars)\n",
    "\n",
    "\n",
    "y = model.predict(diphoMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fb220",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import uproot\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "diphoVars  = ['dipho_leadIDMVA', 'dipho_subleadIDMVA', 'dipho_lead_ptoM', \n",
    "              'dipho_sublead_ptoM', 'dipho_leadEta', 'dipho_subleadEta', \n",
    "              'CosPhi', 'vtxprob', 'sigmarv', 'sigmawv']\n",
    "allVars = diphoVars + [\"dipho_mass\"]\n",
    "\n",
    "f = uproot.open('../data/ggH_powheg_UL_2017.root')\n",
    "tree = f['vbfTagDumper/trees/ggh_125_13TeV_GeneralDipho']\n",
    "arrays = tree.arrays(allVars, how=dict)\n",
    "\n",
    "mask = (  (arrays[\"dipho_mass\"]>100.) & (arrays[\"dipho_mass\"]<180.) \n",
    "        & (arrays[\"dipho_leadIDMVA\"]>-0.9) & (arrays[\"dipho_subleadIDMVA\"]>-0.9) \n",
    "        & (arrays[\"dipho_lead_ptoM\"]>0.333) & (arrays[\"dipho_sublead_ptoM\"]>0.25))\n",
    "\n",
    "x = np.column_stack((ak.to_numpy(arrays[var][mask]) for var in diphoVars))[:100]\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "diphoMatrix = xgboost.DMatrix(x, feature_names=diphoVars)\n",
    "\n",
    "model = xgboost.Booster()\n",
    "model.load_model('aux-data/altDiphoModel_coffea.model')\n",
    "y = model.predict(diphoMatrix)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97315a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.behavior[\"__events_factory__\"]._partition_key.replace('/', '_') + '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e818030",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e11730",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaconditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94b350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows import taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1450390c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DummyTagger1',\n",
       " 'DummyTagger2',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(taggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f975995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
