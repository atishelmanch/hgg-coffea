{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "441aa125",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'workflows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-52d318a5bbc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mworkflows\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDYStudiesProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from coffea.processor import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mrun_uproot_job\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'workflows'"
     ]
    }
   ],
   "source": [
    "from workflows import DYStudiesProcessor\n",
    "\n",
    "import json\n",
    "from coffea.processor import (\n",
    "    run_uproot_job,\n",
    "    iterative_executor, \n",
    "    futures_executor,\n",
    "    NanoAODSchema\n",
    ")\n",
    "\n",
    "with open(\"dummy_samples.json\") as f:\n",
    "    samples = json.load(f)\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "metaconditions_file = \"metaconditions/Era2017_legacy_v1.json\"\n",
    "with open(metaconditions_file) as f:\n",
    "    metaconditions = json.load(f)    \n",
    "\n",
    "dystudies = DYStudiesProcessor(metaconditions, False, True, './outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "histos = run_uproot_job(\n",
    "    samples,\n",
    "    'Events',\n",
    "    dystudies,\n",
    "    executor=futures_executor,\n",
    "    executor_args={\"schema\": NanoAODSchema,\"workers\": 4},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a00af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ak.ones_like(ak.flatten(events.Photon.pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import awkward as ak\n",
    "\n",
    "x = pd.read_parquet('./outputs/')\n",
    "\n",
    "#y = ak.from_arrow(pa.Table.from_pandas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3862b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# di-photon MVA\n",
    "import xgboost as xg\n",
    "\n",
    "min_diphoton_mass = 100\n",
    "max_diphoton_mass = 180\n",
    "\n",
    "model = xg.Booster()\n",
    "model.load_model('aux-data/altDiphoModel_coffea.model')\n",
    "\n",
    "# get the number of diphotons per row\n",
    "# and save for re-wrapping xgb outputs\n",
    "counts = ak.num(diphotons, axis=1)\n",
    "\n",
    "# extract diphoton vars into flat lists\n",
    "dipho_leadIDMVA = ak.flatten(diphotons[\"0\"].mvaID)\n",
    "dipho_subleadIDMVA = ak.flatten(diphotons[\"1\"].mvaID)\n",
    "dipho_lead_ptoM = ak.flatten(diphotons[\"0\"].pt / diphotons.mass)\n",
    "dipho_sublead_ptoM = ak.flatten(diphotons[\"1\"].pt / diphotons.mass)\n",
    "dipho_lead_eta = ak.flatten(diphotons[\"0\"].eta)\n",
    "dipho_sublead_eta = ak.flatten(diphotons[\"1\"].eta)\n",
    "\n",
    "diphoVars  = ['dipho_leadIDMVA', 'dipho_subleadIDMVA', 'dipho_lead_ptoM', \n",
    "              'dipho_sublead_ptoM', 'dipho_leadEta', 'dipho_subleadEta', \n",
    "              'CosPhi', 'vtxprob', 'sigmarv', 'sigmawv']\n",
    "allVars = diphoVars + [\"dipho_mass\"]\n",
    "\n",
    "f = uproot.open('data/ggH_powheg_UL_2017.root')\n",
    "tree = f['vbfTagDumper/trees/ggh_125_13TeV_GeneralDipho']\n",
    "arrays = tree.arrays(allVars, how=dict)\n",
    "\n",
    "mask = (  (arrays[\"dipho_mass\"]> min_diphoton_mass) & (arrays[\"dipho_mass\"]< max_diphoton_mass) \n",
    "        & (arrays[\"dipho_leadIDMVA\"]>-0.9) & (arrays[\"dipho_subleadIDMVA\"]>-0.9) \n",
    "        & (arrays[\"dipho_lead_ptoM\"]>0.333) & (arrays[\"dipho_sublead_ptoM\"]>0.25))\n",
    "\n",
    "x = np.column_stack((ak.to_numpy(arrays[var][mask]) for var in diphoVars))[:100]\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "diphoMatrix = xg.DMatrix(x, feature_names=diphoVars)\n",
    "\n",
    "\n",
    "y = model.predict(diphoMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fbda9aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corrections_summary': 'aux-data/Taggers/PhoIdInputsCorrections/corrections_summary_2017_Legacy_xgb.json', 'SS_variables': ['f0 := pt', 'f1 := eta', 'f2 := phi', 'f3 := fixedGridRhoAll', 'f4 := sieip', 'f5 := s4', 'f6 := r9', 'f7 := phiWidth', 'f8 := sieip', 'f9 := etaWidth']}\n",
      "peak2tail : ['variables', 'weights']\n",
      "peak_tail_clfs : ['weights_data', 'weights_mc', 'variables']\n",
      "morphing : ['variables', 'scale', 'weights', 'center']\n",
      "peak2tail : ['variables', 'weights']\n",
      "peak_tail_clfs : ['weights_data', 'weights_mc', 'variables']\n",
      "morphing : ['variables', 'scale', 'weights', 'center']\n",
      "chIso_peak2tail : ['variables', 'weights']\n",
      "peak_tail_clfs : ['weights_data', 'weights_mc', 'variables']\n",
      "chIsoWorst_peak2tail : ['variables', 'weights']\n",
      "chIsoWorst_morphing : ['variables', 'scale', 'weights', 'center']\n",
      "chIso_morphing : ['variables', 'scale', 'weights', 'center']\n",
      "chIso_peak2tail : ['variables', 'weights']\n",
      "peak_tail_clfs : ['weights_data', 'weights_mc', 'variables']\n",
      "chIsoWorst_peak2tail : ['variables', 'weights']\n",
      "chIsoWorst_morphing : ['variables', 'scale', 'weights', 'center']\n",
      "chIso_morphing : ['variables', 'scale', 'weights', 'center']\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "import warnings\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import json\n",
    "import os.path as osp\n",
    "metaconditions_file = \"metaconditions/Era2017_legacy_xgb_v1.json\"\n",
    "with open(metaconditions_file) as f:\n",
    "    metaconditions = json.load(f)\n",
    "    \n",
    "print(metaconditions[\"PhoIdInputCorrections\"])\n",
    "\n",
    "data_kind = \"mc\"\n",
    "\n",
    "def wrapped_xgb(array, model, scale, center, variables):\n",
    "    asDMatrix = xgboost.DMatrix(array)\n",
    "    return model.predict(asDMatrix)\n",
    "\n",
    "def create_evaluator(weights, scale=None, center=None, variables=None, **kwargs):\n",
    "    try:\n",
    "        model = xgboost.Booster()\n",
    "        model.load_model(weights)\n",
    "        return partial(wrapped_xgb, model=model, scale=scale, center=center, variables=variables)\n",
    "    except xgboost.core.XGBoostError:\n",
    "        raise ValueError(f\"could not find: {weights}\")\n",
    "\n",
    "class ChainedQuantileRegression:\n",
    "    def __init__(\n",
    "        self, \n",
    "        corrections_summary, \n",
    "        SS_variables, \n",
    "        correctShowerShapes=True, \n",
    "        correctIsolations=True, \n",
    "        correctPreshower=False,\n",
    "    ):\n",
    "        with open(corrections_summary, \"r\") as f:\n",
    "            cq_config = json.load(f)\n",
    "        \n",
    "        self.transforms = {}\n",
    "        \n",
    "        for vargroup, varlist in cq_config.items():\n",
    "            self.transforms[vargroup] = {}\n",
    "            for varname, regions in varlist.items():\n",
    "                for region, steps in regions.items():\n",
    "                    if not correctIsolations and vargroup == \"isolations\":\n",
    "                        continue\n",
    "                    if not correctShowerShapes and vargroup == \"shower_shapes\":\n",
    "                        continue\n",
    "                    self.transforms[vargroup][varname] = {}\n",
    "                    if vargroup == \"isolations\":\n",
    "                        self.transforms[vargroup][varname][region] = {}\n",
    "                        for stepname, config in steps.items():\n",
    "                            keys = list(config.keys())\n",
    "                            print(stepname, \":\", keys)\n",
    "                            if \"variables\" in keys:\n",
    "                                config[\"variables\"] = [var.split(\":=\")[-1].strip() for var in config[\"variables\"]]\n",
    "                            if f\"weights_data\" in keys and \"weights_mc\" in keys:\n",
    "                                config[\"weights\"] = config[\"weights_data\"]\n",
    "                                self.transforms[vargroup][varname][region][f\"{stepname}_data\"] = (\n",
    "                                    create_evaluator(**config)\n",
    "                                )\n",
    "                                config[\"weights\"] = config[\"weights_mc\"]\n",
    "                                self.transforms[vargroup][varname][region][f\"{stepname}_mc\"] = (\n",
    "                                    create_evaluator(**config)\n",
    "                                )\n",
    "                            else:\n",
    "                                self.transforms[vargroup][varname][region][stepname] = (\n",
    "                                    create_evaluator(**config)\n",
    "                                )\n",
    "                    elif vargroup == \"shower_shapes\":\n",
    "                        self.transforms[vargroup][varname][region] = []\n",
    "                        self.transforms[vargroup][varname][region].append(\n",
    "                            create_evaluator(**steps)\n",
    "                        )\n",
    "                    else:\n",
    "                        raise Exception(\n",
    "                            f\"{vargroup} is not understood by ChainedQuantileRegression\"\n",
    "                        )\n",
    "        self.ssvs = [ssv.split(\":=\")[-1].strip() for ssv in SS_variables]\n",
    "    \n",
    "    def apply(self, events):\n",
    "        pass\n",
    "            \n",
    "test = ChainedQuantileRegression(**metaconditions[\"PhoIdInputCorrections\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad841741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function wrapped_xgb at 0x7f7499f0f9d0>, model=<xgboost.core.Booster object at 0x7f7499f27ca0>, scale=0.08328429357760053, center=0.04449992030025607, variables=['pt', 'superCluster.eta', 'phi', 'global.rho', 'pfPhoIso03'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.transforms[\"isolations\"][\"phoIso\"][\"EB\"][\"morphing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "x = pickle.dumps(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lz4.frame\n",
    "\n",
    "print(len(x))\n",
    "print(len(lz4.frame.compress(x, compression_level=9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcdfb949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weightFile': 'aux-data/Taggers/STXSmodels/altDiphoModel_coffea.json', 'version': 'xgb', 'inputs': ['dipho_leadIDMVA', 'dipho_subleadIDMVA', 'dipho_lead_ptoM', 'dipho_sublead_ptoM', 'dipho_leadEta', 'dipho_subleadEta', 'CosPhi', 'vtxprob', 'sigmarv', 'sigmawv']}\n",
      "var calc 0.26253880001604557\n",
      "dmatrix 0.02345369989052415\n",
      "predict 0.012677599908784032\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import xgboost\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import vector\n",
    "from coffea import nanoevents\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "metaconditions_file = \"metaconditions/Era2017_legacy_xgb_v1.json\"\n",
    "with open(metaconditions_file) as f:\n",
    "    metaconditions = json.load(f)\n",
    "    \n",
    "print(metaconditions[\"flashggDiPhotonMVA\"])\n",
    "var_order = metaconditions[\"flashggDiPhotonMVA\"][\"inputs\"]\n",
    "\n",
    "# get some events to play with\n",
    "events = nanoevents.NanoEventsFactory.from_root(\"../data/step2_HggNano_test.root\").events()\n",
    "diphotons = ak.combinations(events.Photon, 2)\n",
    "\n",
    "p4s = diphotons.slot0 + diphotons.slot1\n",
    "diphotons[\"pt\"] = p4s.pt\n",
    "diphotons[\"eta\"] = p4s.eta\n",
    "diphotons[\"phi\"] = p4s.phi\n",
    "diphotons[\"mass\"] = p4s.mass\n",
    "diphotons[\"charge\"] = p4s.charge\n",
    "diphotons = ak.with_name(diphotons, \"PtEtaPhiMCandidate\")\n",
    "\n",
    "bdt_vars = {}\n",
    "\n",
    "bdt_vars[\"dipho_leadIDMVA\"] = diphotons.slot0.mvaID\n",
    "bdt_vars[\"dipho_subleadIDMVA\"] = diphotons.slot1.mvaID\n",
    "bdt_vars[\"dipho_leadEta\"] = diphotons.slot0.eta\n",
    "bdt_vars[\"dipho_subleadEta\"] = diphotons.slot1.eta\n",
    "bdt_vars[\"dipho_lead_ptoM\"] = diphotons.slot0.pt / diphotons.mass\n",
    "bdt_vars[\"dipho_sublead_ptoM\"] = diphotons.slot1.pt / diphotons.mass\n",
    "\n",
    "# calculate sigma_wv\n",
    "def calc_displacement(photons, events):\n",
    "    x = photons.x_calo - events.PV.x\n",
    "    y = photons.y_calo - events.PV.y\n",
    "    z = photons.z_calo - events.PV.z\n",
    "    return ak.zip({\"x\": x, \"y\": y, \"z\": z}, with_name=\"Vector3D\")\n",
    "\n",
    "tic = time.monotonic()\n",
    "v_lead = calc_displacement(diphotons.slot0, events)\n",
    "v_sublead = calc_displacement(diphotons.slot1, events)\n",
    "\n",
    "p_lead = v_lead.unit() * diphotons.slot0.energyRaw\n",
    "p_lead[\"energy\"] = diphotons.slot0.energyRaw\n",
    "p_lead = ak.with_name(p_lead, \"Momentum4D\")\n",
    "p_sublead = v_sublead.unit() * diphotons.slot1.energyRaw\n",
    "p_sublead[\"energy\"] = diphotons.slot1.energyRaw\n",
    "p_sublead = ak.with_name(p_sublead, \"Momentum4D\")\n",
    "\n",
    "sech_lead = 1.0 / np.cosh(p_lead.eta)\n",
    "sech_sublead = 1.0/ np.cosh(p_sublead.eta)\n",
    "tanh_lead = np.cos(p_lead.theta)\n",
    "tanh_sublead = np.cos(p_sublead.theta)\n",
    "\n",
    "cos_dphi = np.cos(p_lead.deltaphi(p_sublead))\n",
    "\n",
    "numerator_lead = sech_lead * (sech_lead * tanh_sublead - tanh_lead * sech_sublead * cos_dphi)\n",
    "numerator_sublead = sech_sublead * (sech_sublead * tanh_lead - tanh_sublead * sech_lead * cos_dphi)\n",
    "\n",
    "denominator = 1.0 - tanh_lead * tanh_sublead - sech_lead * sech_sublead * cos_dphi\n",
    "\n",
    "add_reso = 0.5 * (-np.sqrt(2.0) * events.BeamSpot.sigmaZ / denominator) * (numerator_lead/p_lead.mag + numerator_sublead/p_sublead.mag)\n",
    "\n",
    "dEnorm_lead = diphotons.slot0.energyErr / diphotons.slot0.energy\n",
    "dEnorm_sublead = diphotons.slot1.energyErr / diphotons.slot1.energy\n",
    "\n",
    "sigma_m = 0.5 * np.sqrt(dEnorm_lead**2 + dEnorm_sublead**2)\n",
    "sigma_wv = np.sqrt(add_reso**2 + sigma_m**2)\n",
    "toc = time.monotonic()\n",
    "\n",
    "print(\"var calc\", toc - tic)\n",
    "\n",
    "\n",
    "vtx_prob = ak.full_like(sigma_m, 0.999) # !!!! placeholder !!!!\n",
    "\n",
    "bdt_vars[\"CosPhi\"] = cos_dphi\n",
    "bdt_vars[\"vtxprob\"] = vtx_prob\n",
    "bdt_vars[\"sigmarv\"] = sigma_m\n",
    "bdt_vars[\"sigmawv\"] = sigma_wv\n",
    "\n",
    "# create the diphoton BDT\n",
    "diphotonMVA = xgboost.Booster()\n",
    "diphotonMVA.load_model(metaconditions[\"flashggDiPhotonMVA\"][\"weightFile\"])\n",
    "\n",
    "counts = ak.num(diphotons, axis=-1)\n",
    "bdt_inputs = np.column_stack([ak.to_numpy(ak.flatten(bdt_vars[name])) for name in var_order])\n",
    "\n",
    "tic = time.monotonic()\n",
    "tempmatrix = xgboost.DMatrix(bdt_inputs, feature_names=var_order)\n",
    "toc = time.monotonic()\n",
    "print(\"dmatrix\", toc - tic)\n",
    "\n",
    "tic = time.monotonic()\n",
    "scores = diphotonMVA.predict(tempmatrix)\n",
    "toc = time.monotonic()\n",
    "print(\"predict\", toc - tic)\n",
    "\n",
    "diphotons[\"bdt_score\"] = ak.unflatten(scores, counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43264c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Photon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3cd66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fb220",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import uproot\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "diphoVars  = ['dipho_leadIDMVA', 'dipho_subleadIDMVA', 'dipho_lead_ptoM', \n",
    "              'dipho_sublead_ptoM', 'dipho_leadEta', 'dipho_subleadEta', \n",
    "              'CosPhi', 'vtxprob', 'sigmarv', 'sigmawv']\n",
    "allVars = diphoVars + [\"dipho_mass\"]\n",
    "\n",
    "f = uproot.open('../data/ggH_powheg_UL_2017.root')\n",
    "tree = f['vbfTagDumper/trees/ggh_125_13TeV_GeneralDipho']\n",
    "arrays = tree.arrays(allVars, how=dict)\n",
    "\n",
    "mask = (  (arrays[\"dipho_mass\"]>100.) & (arrays[\"dipho_mass\"]<180.) \n",
    "        & (arrays[\"dipho_leadIDMVA\"]>-0.9) & (arrays[\"dipho_subleadIDMVA\"]>-0.9) \n",
    "        & (arrays[\"dipho_lead_ptoM\"]>0.333) & (arrays[\"dipho_sublead_ptoM\"]>0.25))\n",
    "\n",
    "x = np.column_stack((ak.to_numpy(arrays[var][mask]) for var in diphoVars))[:100]\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "diphoMatrix = xgboost.DMatrix(x, feature_names=diphoVars)\n",
    "\n",
    "model = xgboost.Booster()\n",
    "model.load_model('aux-data/altDiphoModel_coffea.model')\n",
    "y = model.predict(diphoMatrix)\n",
    "\n",
    "print(type(x))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97315a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.behavior[\"__events_factory__\"]._partition_key.replace('/', '_') + '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e818030",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e11730",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaconditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows import taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(taggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f975995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "x = ak.Array([[1.0], [2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cfadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ak.num(x, axis=1)\n",
    "ak.unflatten([0, 0] * ak.flatten(x)[:, None], counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([0,1,2])\n",
    "y = np.array([3,4,5])\n",
    "\n",
    "np.stack((x,y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.Array([1,2,3,4])[ak.min(ak.Array([[1,-1], []]), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167df8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(['abc', 1, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(\"['abc', 1, 5.0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e8213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(None or 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4001ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
